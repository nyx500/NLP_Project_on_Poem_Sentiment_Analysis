{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd17006a-4966-4323-a4e4-f9f21c566a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adapt document features function to account for bigrams combined with SentiWordNet sentiment scores.\n",
    "\n",
    "def doc_features_with_swn_sentiment_scores_bigrams(document, word_features): \n",
    "    \n",
    "    # Get the SentiWordNet pos and neg summed scores for the document (a list of tokens)\n",
    "    positive_score, negative_score = get_sentiwordnet_scores_from_tokens(document)\n",
    "  \n",
    "    # Remove duplicate words from the document (a tokenized line of poetry)\n",
    "    document_words = set(document)  \n",
    "    # Extract the set of bigrams from the document\n",
    "    document_bigrams = set(bigrams(document))\n",
    "    \n",
    "    # Create a features dictionary to represent the word features\n",
    "    features = {}\n",
    "    # Iterate over the top N vocabulary words (word_features) and create a dict-key for that word, with the dict-value signalling whether the\n",
    "    # word or bigram occurs in the document (line of poetry) or not.\n",
    "    # Also add new key-value pairs to the features dictionary indicating the sentiment scores of the word in question\n",
    "    for word in word_features:\n",
    "        features[f\"contains({word})\"] = (word in document_words or word in document_bigrams)\n",
    "\n",
    "    # Add features representing swn positive and negative scores\n",
    "    features[\"positive_sentiment\"] = positive_score\n",
    "    features[\"negative_sentiment\"] = negative_score\n",
    "    \n",
    "    return features\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    A function that takes in a range of values for the different nr of most common words to use as features\n",
    "    and then calculates the accuracies and average f1-scores for each nr of most common words.\n",
    "    In contrast to the `afinn_calculate_metrics_for_different_vocab_size_features`, this function\n",
    "    uses `doc_features_with_swn_sentiment_scores`  instead of the `doc_features_with_afinn_sentiment_scores` function\n",
    "    to use SentiWordNet summed positive and negative scores instead of AFINN sentiment scores.\n",
    "    This version of the function also includes bigrams as features.\n",
    "\"\"\"\n",
    "def swn_calculate_metrics_for_different_vocab_size_features_with_bigrams(\n",
    "    lowest_num_words_limit, # lower end of range for how many words to use\n",
    "    highest_num_words_limit, # higher end of range for how many words to use\n",
    "    all_grams, # the freq dist of unigrams and bigrams ordered by most common to least common\n",
    "    train_tuples, # training data tuples of form (sample, label)\n",
    "    val_tuples,\n",
    "    step_size=50, # interval size between numbers of words to test\n",
    "):\n",
    "    top_word_counts = np.arange(lowest_num_words_limit, highest_num_words_limit, step_size)\n",
    "    accuracies = [] # store accuracies for each nr of top words used in here\n",
    "    avg_f1_scores = [] # store macro f1 scores for each nr of top words used in here\n",
    "\n",
    "    # iterate over the array of top-word counts to use (i.e. vocab subset to use in features)\n",
    "    for vocab_size in top_word_counts:\n",
    "        print(vocab_size)\n",
    "        # store the list of top \"vocab_size\" words to use\n",
    "        word_features = list(all_grams)[:vocab_size]\n",
    "        # get the featuresets based on the top N word features for training and validation splits\n",
    "        train_data_featuresets = [(doc_features_with_swn_sentiment_scores_bigrams(doc, word_features), label) for (doc, label) in train_tuples]\n",
    "        validation_data_featuresets = [(doc_features_with_swn_sentiment_scores_bigrams(doc, word_features), label) for (doc, label) in val_tuples]\n",
    "        # train a NB classifier and append accuracy score to the above-defined list\n",
    "        NBclassifier = nltk.NaiveBayesClassifier.train(train_data_featuresets)\n",
    "        accuracy = nltk.classify.accuracy(NBclassifier, validation_data_featuresets)\n",
    "        accuracies.append(accuracy)\n",
    "        # # now get the macro avg f1 scores (more complicated)\n",
    "        # # store all the predicted labels here\n",
    "        validation_predictions= []\n",
    "\n",
    "        # Iterate over each validation featurset and get the predicted label\n",
    "        for features_dict, label in validation_data_featuresets:\n",
    "            predicted_label = NBclassifier.classify(features_dict)\n",
    "            validation_predictions.append(predicted_label)\n",
    "    \n",
    "        # Retrieve the macro-average F1 score from classification report and store it in avg_f1_scores\n",
    "        class_report = classification_report(\n",
    "            original_dataset_validation_labels, \n",
    "            validation_predictions,\n",
    "            output_dict=True,  # Return report as a dictionary (easier to access metrics)\n",
    "            # Set the score to 0 if \"UndefinedMetricWarning\" appears because either recall or precision for a class are 0.0\n",
    "            zero_division=0  \n",
    "        )\n",
    "        \n",
    "        macro_avg_f1 = class_report['macro avg']['f1-score'] \n",
    "        avg_f1_scores.append(macro_avg_f1)\n",
    "    return top_word_counts, accuracies, avg_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1de73c9-5b38-43f4-a1d8-a08086b5b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_negated_train_tokens_with_bigrams = [list(bigrams(sample)) + sample for sample in wordnet_negated_train_tokens]\n",
    "wordnet_negated_validation_tokens_with_bigrams = [list(bigrams(sample)) + sample for sample in wordnet_negated_validation_tokens]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
