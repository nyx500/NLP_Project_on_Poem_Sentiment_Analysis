{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43916db-d50d-4729-9552-ed1a944b432b",
   "metadata": {},
   "source": [
    "# Bayesian Optimization links\n",
    "\n",
    "- https://distill.pub/2020/bayesian-optimization/\n",
    "- https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize-150c1b1d4ba\n",
    "- https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html\n",
    "- https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b\n",
    "- https://medium.datadriveninvestor.com/k-fold-cross-validation-for-parameter-tuning-75b6cb3214f\n",
    "- https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "- https://huggingface.co/blog/ray-tune\n",
    "- https://wandb.ai/amogkam/transformers/reports/Hyperparameter-Optimization-for-HuggingFace-Transformers--VmlldzoyMTc2ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7391f0-9244-4ca9-aea4-1ac4a57aa0eb",
   "metadata": {},
   "source": [
    "## [Link 1 (Agnihotri & Batra, 2020)](https://distill.pub/2020/bayesian-optimization/)\n",
    "\n",
    "- Gold Mining problem: \"In this problem, we want to accurately estimate the gold distribution on the new land. We can not drill at every location due to the prohibitive cost. Instead, we should drill at locations providing high information about the gold distribution. This problem is akin to Active Learning\n",
    "[2, 3]\"\n",
    "- \"In this problem, we want to find the location of the maximum gold content. We, again, can not drill at every location. Instead, we should drill at locations showing high promise about the gold content.\" \"\"sthing called an acquisition function. Acquisition functions are heuristics for how desirable it is to evaluate a point, based on our present model 4 . We will spend much of this section going through different options for acquisition functions.\n",
    "This brings us to how Bayesian Optimization works. At every step, we determine what the best point to evaluate next is according to the acquisition function by optimizing it. We then update our model and repeat this process to determine the next point to evaluate.\n",
    "You may be wondering what’s “Bayesian” about Bayesian Optimization if we’re just optimizing these acquisition functions. Well, at every step we maintain a model describing our estimates and uncertainty at each point, which we update according to Bayes’ rule\n",
    "[2] at each step. Our acquisition functions are based on this model, and nothing would be possible without them.\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ca9d2-416d-470d-8cf1-8b7e19342ab9",
   "metadata": {},
   "source": [
    "\"When training a model is not expensive and time-consuming, we can do a grid search to find the optimum hyperparameters. However, grid search is not feasible if function evaluations are costly, as in the case of a large neural network that takes days to train. Further, grid search scales poorly in terms of the number of hyperparameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e04ae-d3fb-4ae2-92d3-1078670b7de8",
   "metadata": {},
   "source": [
    "\"Next, we looked at the “Bayes” in Bayesian Optimization — the function evaluations are used as data to obtain the surrogate posterior. We look at acquisition functions, which are functions of the surrogate posterior and are optimized sequentially. This new sequential optimization is in-expensive and thus of utility of us. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246d02d-18ec-44cc-ab3d-380a40f71126",
   "metadata": {},
   "source": [
    "## [Link 2 (Alvarez, 2023)](https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize-150c1b1d4ba)\n",
    "\n",
    "-  #simple to do params section\r\n",
    "    search_space = {'lr': np.arange(.001, .0501, .0001),\r\n",
    "                    'layers': np.arange(1, max_layers+1),\r\n",
    "                    'batch_size': [16, 32, 64, 128],\r\n",
    "                    'optimizers': [RMSprop, Adam, Nadam]\r\n",
    "                    #SGD causes the exploding gradient problem in regression\r\n",
    "              \n",
    "\n",
    "-      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e022f58-c23a-4e0a-b99f-8244df795b8a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Agnihotri, A., & Batra, N. (2020). Exploring Bayesian Optimization. Distill. Retrieved June 25, 2024, from https://distill.pub/2020/bayesian-optimization/\n",
    "\n",
    "[2] \r\n",
    "Alvarez, J. (2023, June 7). Tuning deep learning made easy with Scikit-Optimize. *LatinXinAI*. Retrieved June 25, 2024, from https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf6c05f9-575d-416a-b420-5530714ba2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ophel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ophel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ophel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\ophel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the \"datasets\" library that allows downloading datasets from Hugging Face\n",
    "#!pip install datasets # datasets library is already installed on this machine.\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "# Enable loading the local copy of the dataset with this function\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "# Download NLP libraries\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "# Word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import copy\n",
    "\n",
    "## Import wordnet functionality for negation handling\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import libraries for plotting confusion matrices\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### Scikit-Learn functionality\n",
    "# TF-IDF functionality\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import evaluation metrics\n",
    "from nltk.classify import accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score,  precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import train_test_split for stratified dataset splitting to maintain proportions of each class in each split for cross-val part of the coursework\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Allow stratified k-fold cross-validation to address challenges of dealing with an unbalanced dataset.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Import sentiment lexicons\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8c0fc4b-12ca-4ab5-ab4a-0a8828ed3be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 892\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 105\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'verse_text', 'label'],\n",
      "        num_rows: 104\n",
      "    })\n",
      "})\n",
      "TRAIN DATA\n",
      "   id                                         verse_text  label\n",
      "0   0  with pale blue berries. in these peaceful shad...      1\n",
      "1   1                it flows so long as falls the rain,      2\n",
      "2   2                 and that is why, the lonesome day,      0\n",
      "3   3  when i peruse the conquered fame of heroes, an...      3\n",
      "4   4            of inward strife for truth and liberty.      3\n",
      "5   5                   the red sword sealed their vows!      3\n",
      "6   6                          and very venus of a pipe.      2\n",
      "7   7                who the man, who, called a brother.      2\n",
      "8   8           and so on. then a worthless gaud or two,      0\n",
      "9   9         to hide the orb of truth--and every throne      2\n",
      "\n",
      "**********************************************************************\n",
      "\n",
      "VALIDATION DATA\n",
      "   id                                         verse_text  label\n",
      "0   0              to water, cloudlike on the bush afar,      2\n",
      "1   1      shall yet be glad for him, and he shall bless      1\n",
      "2   2  on its windy site uplifting gabled roof and pa...      2\n",
      "3   3                    (if haply the dark will of fate      0\n",
      "4   4                            jehovah, jove, or lord!      2\n",
      "5   5         when the brow is cold as the marble stone,      0\n",
      "6   6         taking and giving radiance, and the slopes      1\n",
      "7   7                     press hard the hostile towers!      0\n",
      "8   8     his head is bowed. he thinks on men and kings.      2\n",
      "9   9                   with england if the day go hard,      2\n",
      "\n",
      "**********************************************************************\n",
      "\n",
      "TEST DATA\n",
      "   id                                         verse_text  label\n",
      "0   0                      my canoe to make more steady,      2\n",
      "1   1  and be glad in the summer morning when the kin...      1\n",
      "2   2       and when they reached the strait symplegades      2\n",
      "3   3                             she sought for flowers      2\n",
      "4   4                       if they are hungry, paradise      2\n",
      "5   5                      indignantly i hurled the cry:      0\n",
      "6   6                   with which his house is haunted;      0\n",
      "7   7    and, laying snow-white flowers against my hair.      2\n",
      "8   8          of long-uncoupled bed, and childless eld,      2\n",
      "9   9  of the boulder-strewn mountain, and when they ...      2\n"
     ]
    }
   ],
   "source": [
    "# Load the HuggingFace poem dataset from local storage into a variable called \"dataset\"\n",
    "original_dir = './datasets/original_poem_sentiment_dataset'\n",
    "poem_dataset = load_from_disk(original_dir)\n",
    "print(f\"Original dataset: {poem_dataset}\")\n",
    "\n",
    "train_ds = poem_dataset['train']\n",
    "val_ds = poem_dataset['validation']\n",
    "test_ds = poem_dataset['test']\n",
    "\n",
    "# Convert the three splits into pandas dataframes for easier viewing and analysis of the dataset using the inbuilt 'to_pandas' method\n",
    "train_df = train_ds.to_pandas() \n",
    "val_df = val_ds.to_pandas()\n",
    "test_df = test_ds.to_pandas()\n",
    "\n",
    "# Display the first 10 lines in each of the training, val and test data splits\n",
    "print(\"TRAIN DATA\")\n",
    "print(train_df.head(10))\n",
    "print('\\n**********************************************************************\\n')\n",
    "print(\"VALIDATION DATA\")\n",
    "print(val_df.head(10))\n",
    "print('\\n**********************************************************************\\n')\n",
    "print(\"TEST DATA\")\n",
    "print(test_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde100ea-8b31-45ef-a5fe-a0a396b78eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n",
      "{'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n",
      "{'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.features)\n",
    "print(val_ds.features)\n",
    "print(test_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2219c-00d4-4544-b573-df459be5d4b6",
   "metadata": {},
   "source": [
    "### Tunstall, von Werra and Thomas Wolf: Natural Language Processing with Transformers --> Building Language Applications with Hugging Face (ch. 2)\n",
    "\n",
    "- DistilBERT cannot receive raw strings as input, the texts must be tokenized and \"encoded\".\n",
    "- Tokenization: breaking down a string into the atomic units used in the model.\n",
    "- The optimal splitting of words into subunits is **usually learned from the corpus**.\n",
    "- p.33 on subword tokenization, dealing with complex words/mispellings by combining the best aspects of character and word tokenization --> it is \"learned\" from the pre-training corpus using a mix of rule-based and statistical algorithms.\n",
    "- WordPiece is used by DistilBERT tokenizers.\n",
    "- \"HuggingFace Trans\n",
    "formers provides a convenient AutoTokenizer class that allows you to quickly loa \r\n",
    "the tokenizer associated with a pretrained model—we just call its from_pretrained )\r\n",
    "method, providing the ID of a model on the Hub or a local file path. Let’s start by\r\n",
    "loading the tokenizer for Distil.\" (p.33)\n",
    "- BRT \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e501085-87b6-458a-93c4-7e60c01d6629",
   "metadata": {},
   "source": [
    "### DistilBERT Link and Description\n",
    "- Link: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "- \"DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts using the BERT base model.\"\n",
    "- \"Distillation loss: the model was trained to return the same probabilities as the BERT base model.\"\n",
    "- \"it's mostly intended to be fine-tuned on a downstream task.\"\n",
    "- \"Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.\"\n",
    "- \"DistilBERT pretrained on the same data as BERT, which is BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers).\"\n",
    "- The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form: \"[CLS] Sentence A [SEP] Sentence B [SEP]\"\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eeed08-554e-41eb-9d59-c835d734cebf",
   "metadata": {},
   "source": [
    "### Getting Started with Sentiment Analysis using Python\n",
    "\n",
    "https://huggingface.co/blog/sentiment-analysis-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f3e0f-954a-47cd-bc19-41ca85761219",
   "metadata": {},
   "source": [
    "### Hyperparameter Search with Transformers and Ray Tune\n",
    "## Code adapted from: https://huggingface.co/blog/ray-tune\n",
    "\n",
    "https://huggingface.co/blog/ray-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05d2391f-1cdf-44db-bf9f-d61ade02bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d44cb140-b828-4ec9-96e8-9187e8584246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM: https://python.plainenglish.io/fine-tuning-distilbert-with-your-own-dataset-for-multi-classification-task-69f944189648\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a64311f-f5d2-47c7-b07c-de822c805a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                         verse_text  label\n",
      "0   0                      my canoe to make more steady,      2\n",
      "1   1  and be glad in the summer morning when the kin...      1\n",
      "2   2       and when they reached the strait symplegades      2\n",
      "3   3                             she sought for flowers      2\n",
      "4   4                       if they are hungry, paradise      2\n",
      "Maximum sequence length: 20\n",
      "Maximum sequence length: 27\n",
      "Maximum sequence length: 20\n"
     ]
    }
   ],
   "source": [
    "# Load in original train-val-test datasets and extract samples and labels\n",
    "train_set = pd.read_csv(\"original_test_df.csv\")\n",
    "val_set = pd.read_csv(\"original_val_df.csv\")\n",
    "test_set = pd.read_csv(\"original_test_df.csv\")\n",
    "print(train_set.head(5))\n",
    "# Convert from pandas Series to list as this is what the distilbert tokenizer requires as inputs\n",
    "train_texts = train_set[\"verse_text\"].to_list()\n",
    "train_labels=train_set[\"label\"].to_list()\n",
    "val_texts = val_set[\"verse_text\"].to_list()\n",
    "val_labels = val_set[\"label\"].to_list()\n",
    "test_texts = test_set[\"verse_text\"].to_list()\n",
    "test_labels = test_set[\"label\"].to_list()\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Find longest token length in dataset\n",
    "max_length = 0\n",
    "\n",
    "# Tokenize each text sample and find max length\n",
    "for text in train_set[\"verse_text\"]:\n",
    "    # Tokenize text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True) # special tokens = indicate start of sequence, end of sequence, separation\n",
    "    # Update max length\n",
    "    max_length = max(max_length, len(input_ids))\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "max_length = 0\n",
    "# Tokenize each val sample and find max length\n",
    "for text in val_set[\"verse_text\"]:\n",
    "    # Tokenize text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True) # special tokens = indicate start of sequence, end of sequence, separation\n",
    "    # Update max length\n",
    "    max_length = max(max_length, len(input_ids))\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "max_length = 0\n",
    "# Tokenize each val sample and find max length\n",
    "for text in test_set[\"verse_text\"]:\n",
    "    # Tokenize text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True) # special tokens = indicate start of sequence, end of sequence, separation\n",
    "    # Update max length\n",
    "    max_length = max(max_length, len(input_ids))\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76760c53-db65-4694-b25d-abe5eca12ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[101, 2010, 2132, 2003, 11489, 1012, 2002, 6732, 2006, 2273, 1998, 5465, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] of long - uncoupled bed, and childless eld, [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "## Code from: https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "\"\"\"\n",
    "    Fine-tuning in the HuggingFace's transformers library involves using a pre-trained model and a tokenizer \n",
    "    that is compatible with that model's architecture and input requirements.\n",
    "    Each pre-trained model in transformers can be accessed using the right model\n",
    "    class and be used with the associated tokenizer class. Since we want to use \n",
    "    DistilBert for a classification task, we will use the DistilBertTokenizer tokenizer \n",
    "    class to tokenize our texts and then use TFDistilBertForSequenceClassification \n",
    "    model class in a later section to fine-tune the pre-trained model using the output from the tokenizer.\n",
    "    The DistilBertTokenizer generates input_ids and attention_mask as outputs.\n",
    "    This is what is required by a DistilBert model as its inputs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "print(train_encodings.keys())\n",
    "# Code from: https://medium.com/@raoashish10/fine-tuning-a-pre-trained-bert-model-for-classification-using-native-pytorch-c5f33e87616e\n",
    "print(val_encodings['input_ids'][8])\n",
    "print(val_encodings['attention_mask'][8])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f80ab-0e54-439e-bc91-be437589acce",
   "metadata": {},
   "source": [
    "From https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "\n",
    "*\"So, in the above code, we defined the tokenizer object using the from_pretrained() method which downloads and caches the tokenizer files associated with the DistilBert model. When we pass text through this tokenizer the generated output will be in the format expected by the DistilBert architecture, as stated above. We use padding and truncation to make sure all the vectors are the same size. You can learn more about DistilBert and it's tokenizer from the DistilBert section of the transformers library's official documentation. And more info regarding the padding and truncation options is available here. Now that we have our texts in an encoded form, there is only one step left before we can begin the fine-tuning process.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc870b51-3f54-47b6-8cdc-ca6e6c65cff9",
   "metadata": {},
   "source": [
    "## Fine-Tuning using TFTrainer Class Provided by the *transformers* Library: enables easy training and evaluation of models\n",
    "\n",
    "\n",
    "From https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "- The TFTrainer (Trainer for Pytorch) is a class provided by the transformers library that offers a simple, yet feature-rich, method of training and evaluating models.\n",
    "- The following code shows how to define the configuration settings and build a model using the TFTrainer class.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0de4fccd-1d2d-423c-8674-160c2b51707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my canoe to make more steady, 2 \n",
      "\n",
      "{'input_ids': tensor([  101,  2026, 14347,  2000,  2191,  2062,  6706,  1010,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(2)}\n"
     ]
    }
   ],
   "source": [
    "# Code from: https://huggingface.co/transformers/v3.4.0/custom_datasets.html\n",
    "\"\"\" We put the data in this format so that the data can be easily batched such that each key in the batch encoding\n",
    "    corresponds to a named parameter of the forward() method of the model we will train. \"\"\"\n",
    "\n",
    "# Create a custom dataset class inheriting from PyTorch's Dataset class --> required as inputs to the DistilbertModel\n",
    "class PoemDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Store list of encoded tokens here\n",
    "        self.encodings = encodings\n",
    "        # Store list of corresponding labels for each sample here\n",
    "        self.labels = labels\n",
    "        \n",
    "    # This special __ getter function enables retrieving items in an encoding using []-notation and 'idx' as the integer to index into the \n",
    "    # encodings\n",
    "    def __getitem__(self, idx):\n",
    "        # dict comprehension: creates a key for each key in the encoding for a specific idx/sample: e.g., 'input_ids'\n",
    "        # the values will be the list containing the encoded tokens, attention masks etc.\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # add key-value pair for label of indexed sample\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    # return size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PoemDataset(train_encodings, train_labels)\n",
    "val_dataset = PoemDataset(val_encodings, val_labels)\n",
    "test_dataset = PoemDataset(test_encodings, test_labels)\n",
    "\n",
    "# Verify this has worked by taking the first train sample as an example\n",
    "print(train_texts[0], train_labels[0], '\\n')\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d945b9f-1723-42a1-8095-b493de4ce226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://medium.com/@rakeshrajpurohit/customized-evaluation-metrics-with-hugging-face-trainer-3ff00d936f99\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    true_labels = pred.label_ids\n",
    "    predicted_labels = pred.predictions.argmax(-1)\n",
    "    # select metrics for dealing with unbalanced classes (macro precision/recall/f1-score)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1-score': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d20f2-84be-4a3e-a56e-231a1d13b139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7/21 00:07 < 00:19, 0.71 it/s, Epoch 0.86/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now all we need to do is create a model to fine-tune, define the TrainingArguments/TFTrainingArguments and instantiate a Trainer/TFTrainer.\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize model with correct number of labels\n",
    " # this line will result in warning: You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
    "# this is a warning telling us to fine-tune the model on our own dataset before proceeding with evaluating on test set, which we will now do.\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=model,                      # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,               # training arguments, defined above\n",
    "    train_dataset=train_dataset,      # training dataset\n",
    "    eval_dataset=val_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics    # compute macro-avg precision/recall/f1-scores\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f27178f-3992-44d7-9c4c-075e10c68027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test dataset\n",
    "eval_results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "# Print evaluation results\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c6ff8-b0b9-4d9c-9af8-731c1e772c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
