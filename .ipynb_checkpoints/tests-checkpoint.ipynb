{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43916db-d50d-4729-9552-ed1a944b432b",
   "metadata": {},
   "source": [
    "# Bayesian Optimization links\n",
    "\n",
    "- https://distill.pub/2020/bayesian-optimization/\n",
    "- https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize-150c1b1d4ba\n",
    "- https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html\n",
    "- https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b\n",
    "- https://medium.datadriveninvestor.com/k-fold-cross-validation-for-parameter-tuning-75b6cb3214f\n",
    "- https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "- https://huggingface.co/blog/ray-tune\n",
    "- https://wandb.ai/amogkam/transformers/reports/Hyperparameter-Optimization-for-HuggingFace-Transformers--VmlldzoyMTc2ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7391f0-9244-4ca9-aea4-1ac4a57aa0eb",
   "metadata": {},
   "source": [
    "## [Link 1 (Agnihotri & Batra, 2020)](https://distill.pub/2020/bayesian-optimization/)\n",
    "\n",
    "- Gold Mining problem: \"In this problem, we want to accurately estimate the gold distribution on the new land. We can not drill at every location due to the prohibitive cost. Instead, we should drill at locations providing high information about the gold distribution. This problem is akin to Active Learning\n",
    "[2, 3]\"\n",
    "- \"In this problem, we want to find the location of the maximum gold content. We, again, can not drill at every location. Instead, we should drill at locations showing high promise about the gold content.\" \"\"sthing called an acquisition function. Acquisition functions are heuristics for how desirable it is to evaluate a point, based on our present model 4 . We will spend much of this section going through different options for acquisition functions.\n",
    "This brings us to how Bayesian Optimization works. At every step, we determine what the best point to evaluate next is according to the acquisition function by optimizing it. We then update our model and repeat this process to determine the next point to evaluate.\n",
    "You may be wondering what’s “Bayesian” about Bayesian Optimization if we’re just optimizing these acquisition functions. Well, at every step we maintain a model describing our estimates and uncertainty at each point, which we update according to Bayes’ rule\n",
    "[2] at each step. Our acquisition functions are based on this model, and nothing would be possible without them.\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ca9d2-416d-470d-8cf1-8b7e19342ab9",
   "metadata": {},
   "source": [
    "\"When training a model is not expensive and time-consuming, we can do a grid search to find the optimum hyperparameters. However, grid search is not feasible if function evaluations are costly, as in the case of a large neural network that takes days to train. Further, grid search scales poorly in terms of the number of hyperparameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e04ae-d3fb-4ae2-92d3-1078670b7de8",
   "metadata": {},
   "source": [
    "\"Next, we looked at the “Bayes” in Bayesian Optimization — the function evaluations are used as data to obtain the surrogate posterior. We look at acquisition functions, which are functions of the surrogate posterior and are optimized sequentially. This new sequential optimization is in-expensive and thus of utility of us. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246d02d-18ec-44cc-ab3d-380a40f71126",
   "metadata": {},
   "source": [
    "## [Link 2 (Alvarez, 2023)](https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize-150c1b1d4ba)\n",
    "\n",
    "-  #simple to do params section\r\n",
    "    search_space = {'lr': np.arange(.001, .0501, .0001),\r\n",
    "                    'layers': np.arange(1, max_layers+1),\r\n",
    "                    'batch_size': [16, 32, 64, 128],\r\n",
    "                    'optimizers': [RMSprop, Adam, Nadam]\r\n",
    "                    #SGD causes the exploding gradient problem in regression\r\n",
    "              \n",
    "\n",
    "-      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e022f58-c23a-4e0a-b99f-8244df795b8a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Agnihotri, A., & Batra, N. (2020). Exploring Bayesian Optimization. Distill. Retrieved June 25, 2024, from https://distill.pub/2020/bayesian-optimization/\n",
    "\n",
    "[2] \r\n",
    "Alvarez, J. (2023, June 7). Tuning deep learning made easy with Scikit-Optimize. *LatinXinAI*. Retrieved June 25, 2024, from https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6c05f9-575d-416a-b420-5530714ba2af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
