{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17330238-f184-4fa4-b3b9-6209073b1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_train_data_nltkNaiveBayes(samples, labels, num_word_features, get_features, k=5):\n",
    "    \"\"\"\n",
    "        Description: A function which applies k-fold cross-validation to the training split of the poem dataset,\n",
    "        and outputs the nltk Multinomial Naive Bayes classifier's mean performance scores across the folds, as well\n",
    "        as the variability in f1-scores (standard deviation) across the folds.\n",
    "        Inputs:\n",
    "            - samples ==> a list-of-lists where each sub-list/sample is a list of tokens.\n",
    "            - labels ==> a list of labels corresponding to each training sample.\n",
    "            - num_word_features ==> num of word features to use for each training-split FreqDist.\n",
    "            - get_features ==> a function (e.g. doc_features) that turns tokens into features using the specific number of word features.\n",
    "            - k ==> an integer representing the number of folds to iterate over for k-fold cross-validation \n",
    "        Outputs:\n",
    "            - a dictionary containing keys for the average accuracy, macro-average precision/recall/F1-scores\n",
    "              and f1 standard deviation across the samples\n",
    "    \"\"\"\n",
    "    # Initialize lists of metrics\n",
    "    accuracies = []\n",
    "    macro_avg_precisions = []\n",
    "    macro_avg_recalls = []\n",
    "    macro_avg_f1s = []\n",
    "\n",
    "    # Use Stratified KFold scikit-learn class with k (nr folds): it outputs indices \n",
    "    # Shuffle to reduce impact of specific orderings of the samples\n",
    "    SKFGenerator = StratifiedKFold(n_splits=k, shuffle=True, random_state=3)  # Use random_state for reproducibility and comparison of results\n",
    "\n",
    "    ## LOGGER counting progress made\n",
    "    counter = 1\n",
    "    \n",
    "    # Iterate over the folds using the outputted indices by StratifiedKFold for this dataset\n",
    "    for train_indices, val_indices in SKFGenerator.split(samples, labels):\n",
    "            \n",
    "        # Create train_set and val_set tuple (sample-label) lists for each fold using the stratified k fold's indices.\n",
    "        train_set = [(samples[i], labels[i]) for i in train_indices]\n",
    "        val_set = [(samples[i], labels[i]) for i in val_indices]\n",
    "\n",
    "        train_tokenlists = [(tokenlist) for (tokenlist, label) in train_set]\n",
    "        train_vocabulary_list = flatten_list_of_lists(train_tokenlists)\n",
    "        all_words = nltk.FreqDist(w for w in train_vocabulary_list)\n",
    "        word_features = list(all_words)[:num_word_features]\n",
    "\n",
    "        train_featuresets = [(get_features(doc, word_features), label) for (doc, label) in train_set]\n",
    "        val_featuresets = [(get_features(doc, word_features), label) for (doc, label) in val_set]\n",
    "        \n",
    "        # Train the Naive Bayes Classifier\n",
    "        NBclassifier = nltk.NaiveBayesClassifier.train(train_featuresets)\n",
    "        \n",
    "         # Get the true labels and the predicted labels from the classifier\n",
    "        true = [label for (features, label) in val_featuresets]\n",
    "        pred = [NBclassifier.classify(features) for (features, label) in val_featuresets]\n",
    "\n",
    "        # Calculate the accuracy for this particular fold.\n",
    "        acc = accuracy(NBclassifier , val_featuresets)\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        # Calculate macro_average precision, recall, and f1-score for this fold\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(true, pred, average='macro', zero_division=0) # set to 0 to avoid zero-division error\n",
    "        macro_avg_precisions.append(precision)\n",
    "        macro_avg_recalls.append(recall)\n",
    "        macro_avg_f1s.append(f1)  \n",
    "        # Increment counter for logging outputs\n",
    "        counter += 1\n",
    "        \n",
    "    # Calculate the mean for all metrics across the folds.\n",
    "    mean_accuracy = np.mean(accuracies)\n",
    "    mean_macro_precision = np.mean(macro_avg_precisions)\n",
    "    mean_macro_recall = np.mean(macro_avg_recalls)\n",
    "    mean_macro_f1 = np.mean(macro_avg_f1s)\n",
    "\n",
    "    # Calculate the standard deviation of f1 across the folds\n",
    "    std_macro_f1 = np.std(macro_avg_f1s, ddof=1) # apply Bessel's correction for fold std deviation as this is a small sample, not a pop.\n",
    "    # Calculate the range of macro f1 scores (to put std into context)\n",
    "    range_macro_f1 = np.max(macro_avg_f1s) - np.min(macro_avg_f1s)\n",
    "    print(f\"Number of word features: {num_word_features} --- Macro-Avg F1 standard deviation: {std_macro_f1} --- Macro-Avg F1 Range: {range_macro_f1}\")\n",
    "\n",
    "    return {\n",
    "        \"mean_accuracy\": mean_accuracy,\n",
    "        \"mean_macro_precision\": mean_macro_precision,\n",
    "        \"mean_macro_recall\": mean_macro_recall,\n",
    "        \"mean_macro_f1\": mean_macro_f1,\n",
    "        \"std_macro_f1\": std_macro_f1,\n",
    "        \"range_macro_f1\": range_macro_f1\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790c27d0-1f4d-4b3a-88a7-18ca55b2554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    true_labels = pred.label_ids\n",
    "    predicted_labels = pred.predictions.argmax(-1)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=0.0)\n",
    "    return {\n",
    "        'eval_accuracy': accuracy,\n",
    "        'eval_precision': precision,\n",
    "        'eval_recall': recall,\n",
    "        'eval_f1': f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e7317f-5dbc-4e70-9f64-f62cc16dfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "from ray import tune\n",
    "\n",
    "def train_fn(config):\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./original_dataset_results',\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        learning_rate=config[\"learning_rate\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate(eval_dataset=eval_dataset)\n",
    "    print(eval_results)\n",
    "\n",
    "    metrics = dict(eval_f1=eval_results[\"eval_f1\"])\n",
    "    \n",
    "    ray.train.report(dict(eval_f1=eval_results[\"eval_f1\"]))\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Define hyperparameter search space\n",
    "search_space = {\n",
    "    \"num_train_epochs\": tune.uniform(6, 11),  # Continuous range between 6 and 10\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"warmup_steps\": tune.uniform(0, 1000),  # Continuous range between 0 and 999\n",
    "    \"weight_decay\": tune.uniform(0.001, 0.1),  # Continuous range between 0.001 and 0.1\n",
    "    \"learning_rate\": tune.uniform(1e-5, 5e-5)  # Continuous range between 1e-5 and 5e-5\n",
    "}\n",
    "\n",
    "bayesopt = BayesOptSearch(\n",
    "    metric=\"eval_f1\",\n",
    "    mode=\"max\",\n",
    "    utility_kwargs={\n",
    "        \"kind\": \"ucb\",   # Use Upper Confidence Bound (UCB) utility function\n",
    "        \"kappa\": 2.5,    # UCB parameter for exploration-exploitation trade-off\n",
    "        \"xi\": 0.0        # Expected Improvement (EI) parameter, trade-off between certain and possible improvements\n",
    "    }\n",
    ")\n",
    "\n",
    "asha_scheduler = ASHAScheduler(\n",
    "    max_t=20,  # Max number of epochs\n",
    "    grace_period=5,  # Min number of epochs to run before stopping a trial\n",
    "    reduction_factor=3  # Factor to reduce the number of trials each iteration\n",
    ")\n",
    "\n",
    "# Define a trial_dirname_creator to shorten the path\n",
    "def short_trial_dirname_creator(trial):\n",
    "    return f\"tune_trial_{trial.trial_id}\"\n",
    "\n",
    "# Initialize Ray and run hyperparameter search with Ray Tune\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_fn,\n",
    "    config=search_space,\n",
    "    search_alg=bayesopt,  # Use BayesOptSearch for hyperparameter optimization\n",
    "    scheduler=asha_scheduler,  # Use ASHA scheduler to manage trial execution\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    num_samples=10,\n",
    "    progress_reporter=tune.CLIReporter(metric_columns=[\"eval_f1\"]),\n",
    "    trial_dirname_creator=short_trial_dirname_creator,\n",
    "    local_dir='./ray',\n",
    "    mode=\"max\",\n",
    "    metric=\"eval_f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0badd4-90f3-4ee0-8b1f-713afe2be84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_show_confusion_matrix(\n",
    "        true_labels, predicted_labels,\n",
    "        label_names, # informative class names go here\n",
    "        classifier_description, matrix_color=plt.cm.Greens, # default: green colour-coded confusion matrix\n",
    "        above_threshold_text_color=\"yellow\" # color in which to show the text of counts above the threshold\n",
    "    ):\n",
    "    # Also visualize the errors made for each class using a confusion matrix:\n",
    "    #matrix = confusion_matrix(original_dataset_validation_labels, original_dataset_validation_predictions)\n",
    "    matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=matrix_color)\n",
    "    plt.title(classifier_description)\n",
    "\n",
    "    # Show the color-coding legend\n",
    "    plt.colorbar()\n",
    "    # Add labels for each class to the x- and y-axes\n",
    "    ticks = np.arange(len(label_names)) # get numpy array of numbers from 0 to nr of classes - 1\n",
    "    print(ticks)\n",
    "    plt.xticks(ticks, label_names, rotation=30) # rotate x-axis labels for easier readability\n",
    "    plt.yticks(ticks, label_names)\n",
    "    \n",
    "    # Add a threshold value (max value in the matrix divided by 2) after which the text-color is inverted from black to white (for visibility)\n",
    "    threshold = matrix.max() / 2.\n",
    "    for i in range(matrix.shape[0]): # iterate over nr rows\n",
    "        for j in range(matrix.shape[1]): # iterate over nr cols\n",
    "            # annotate based on cell (row, col), center the annotation/count, and change text color from black if threshold is exceeded\n",
    "            plt.text(\n",
    "                j, i, format(matrix[i, j], 'd'),\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=above_threshold_text_color if matrix[i, j] > threshold else \"black\"\n",
    "            )\n",
    "\n",
    "    # adjust subplot parameters so that the matrix fits in to the figure area\n",
    "    plt.tight_layout()\n",
    "    # label the axes\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    # display the current figure/matrix\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
