{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f43916db-d50d-4729-9552-ed1a944b432b",
   "metadata": {},
   "source": [
    "# Bayesian Optimization links\n",
    "\n",
    "- https://distill.pub/2020/bayesian-optimization/\n",
    "- https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize-150c1b1d4ba\n",
    "- https://scikit-optimize.github.io/stable/modules/generated/skopt.Space.html\n",
    "- https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b\n",
    "- https://medium.datadriveninvestor.com/k-fold-cross-validation-for-parameter-tuning-75b6cb3214f\n",
    "- https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379\n",
    "- https://huggingface.co/blog/ray-tune\n",
    "- https://wandb.ai/amogkam/transformers/reports/Hyperparameter-Optimization-for-HuggingFace-Transformers--VmlldzoyMTc2ODI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7391f0-9244-4ca9-aea4-1ac4a57aa0eb",
   "metadata": {},
   "source": [
    "## [Link 1 (Agnihotri & Batra, 2020)](https://distill.pub/2020/bayesian-optimization/)\n",
    "\n",
    "- Gold Mining problem: \"In this problem, we want to accurately estimate the gold distribution on the new land. We can not drill at every location due to the prohibitive cost. Instead, we should drill at locations providing high information about the gold distribution. This problem is akin to Active Learning\n",
    "[2, 3]\"\n",
    "- \"In this problem, we want to find the location of the maximum gold content. We, again, can not drill at every location. Instead, we should drill at locations showing high promise about the gold content.\" \"\"sthing called an acquisition function. Acquisition functions are heuristics for how desirable it is to evaluate a point, based on our present model 4 . We will spend much of this section going through different options for acquisition functions.\n",
    "This brings us to how Bayesian Optimization works. At every step, we determine what the best point to evaluate next is according to the acquisition function by optimizing it. We then update our model and repeat this process to determine the next point to evaluate.\n",
    "You may be wondering what’s “Bayesian” about Bayesian Optimization if we’re just optimizing these acquisition functions. Well, at every step we maintain a model describing our estimates and uncertainty at each point, which we update according to Bayes’ rule\n",
    "[2] at each step. Our acquisition functions are based on this model, and nothing would be possible without them.\"\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ca9d2-416d-470d-8cf1-8b7e19342ab9",
   "metadata": {},
   "source": [
    "\"When training a model is not expensive and time-consuming, we can do a grid search to find the optimum hyperparameters. However, grid search is not feasible if function evaluations are costly, as in the case of a large neural network that takes days to train. Further, grid search scales poorly in terms of the number of hyperparameters.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26e04ae-d3fb-4ae2-92d3-1078670b7de8",
   "metadata": {},
   "source": [
    "\"Next, we looked at the “Bayes” in Bayesian Optimization — the function evaluations are used as data to obtain the surrogate posterior. We look at acquisition functions, which are functions of the surrogate posterior and are optimized sequentially. This new sequential optimization is in-expensive and thus of utility of us. \""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f246d02d-18ec-44cc-ab3d-380a40f71126",
   "metadata": {},
   "source": [
    "## [Link 2 (Alvarez, 2023)](https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize-150c1b1d4ba)\n",
    "\n",
    "-  #simple to do params section\r\n",
    "    search_space = {'lr': np.arange(.001, .0501, .0001),\r\n",
    "                    'layers': np.arange(1, max_layers+1),\r\n",
    "                    'batch_size': [16, 32, 64, 128],\r\n",
    "                    'optimizers': [RMSprop, Adam, Nadam]\r\n",
    "                    #SGD causes the exploding gradient problem in regression\r\n",
    "              \n",
    "\n",
    "-      }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e022f58-c23a-4e0a-b99f-8244df795b8a",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] Agnihotri, A., & Batra, N. (2020). Exploring Bayesian Optimization. Distill. Retrieved June 25, 2024, from https://distill.pub/2020/bayesian-optimization/\n",
    "\n",
    "[2] \r\n",
    "Alvarez, J. (2023, June 7). Tuning deep learning made easy with Scikit-Optimize. *LatinXinAI*. Retrieved June 25, 2024, from https://medium.com/latinxinai/tuning-deep-learning-made-easy-with-scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7398c5-f4e0-46f7-8f76-10ffa8414d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the \"datasets\" library that allows downloading datasets from Hugging Face\n",
    "#!pip install datasets # datasets library is already installed on this machine.\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "# Enable loading the local copy of the dataset with this function\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Download NLP libraries\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "# Word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import copy\n",
    "\n",
    "## Import wordnet functionality for negation handling\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import libraries for plotting confusion matrices\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### Scikit-Learn functionality\n",
    "# TF-IDF functionality\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import evaluation metrics\n",
    "from nltk.classify import accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score,  precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import train_test_split for stratified dataset splitting to maintain proportions of each class in each split for cross-val part of the coursework\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Allow stratified k-fold cross-validation to address challenges of dealing with an unbalanced dataset.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Import sentiment lexicons\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# Import hyperparameter training libraries\n",
    "import ray\n",
    "from ray.tune.logger import Logger, UnifiedLogger\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the HuggingFace poem dataset from local storage into a variable called \"dataset\"\n",
    "original_dir = './datasets/original_poem_sentiment_dataset'\n",
    "poem_dataset = load_from_disk(original_dir)\n",
    "print(f\"Original dataset: {poem_dataset}\")\n",
    "\n",
    "train_ds = poem_dataset['train']\n",
    "val_ds = poem_dataset['validation']\n",
    "test_ds = poem_dataset['test']\n",
    "\n",
    "# Convert the three splits into pandas dataframes for easier viewing and analysis of the dataset using the inbuilt 'to_pandas' method\n",
    "train_df = train_ds.to_pandas() \n",
    "val_df = val_ds.to_pandas()\n",
    "test_df = test_ds.to_pandas()\n",
    "\n",
    "# Display the first 10 lines in each of the training, val and test data splits\n",
    "print(\"TRAIN DATA\")\n",
    "print(train_df.head(10))\n",
    "print('\\n**********************************************************************\\n')\n",
    "print(\"VALIDATION DATA\")\n",
    "print(val_df.head(10))\n",
    "print('\\n**********************************************************************\\n')\n",
    "print(\"TEST DATA\")\n",
    "print(test_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1c1e0-ce89-4c47-a0b4-9a364fda55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the \"datasets\" library that allows downloading datasets from Hugging Face\n",
    "#!pip install datasets # datasets library is already installed on this machine.\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "# Enable loading the local copy of the dataset with this function\n",
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Download NLP libraries\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "# Word tokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import bigrams\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "import copy\n",
    "\n",
    "## Import wordnet functionality for negation handling\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Import libraries for plotting confusion matrices\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "### Scikit-Learn functionality\n",
    "# TF-IDF functionality\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Import evaluation metrics\n",
    "from nltk.classify import accuracy\n",
    "from sklearn.metrics import accuracy_score, f1_score,  precision_recall_fscore_support, classification_report, confusion_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Import train_test_split for stratified dataset splitting to maintain proportions of each class in each split for cross-val part of the coursework\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Allow stratified k-fold cross-validation to address challenges of dealing with an unbalanced dataset.\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# Import sentiment lexicons\n",
    "from afinn import Afinn\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "# Import hyperparameter training libraries\n",
    "import ray\n",
    "from ray.tune.logger import Logger, UnifiedLogger\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the HuggingFace poem dataset from local storage into a variable called \"dataset\"\n",
    "original_dir = './datasets/original_poem_sentiment_dataset'\n",
    "poem_dataset = load_from_disk(original_dir)\n",
    "print(f\"Original dataset: {poem_dataset}\")\n",
    "\n",
    "train_ds = poem_dataset['train']\n",
    "val_ds = poem_dataset['validation']\n",
    "test_ds = poem_dataset['test']\n",
    "\n",
    "# Convert the three splits into pandas dataframes for easier viewing and analysis of the dataset using the inbuilt 'to_pandas' method\n",
    "train_df = train_ds.to_pandas() \n",
    "val_df = val_ds.to_pandas()\n",
    "test_df = test_ds.to_pandas()\n",
    "\n",
    "# Display the first 10 lines in each of the training, val and test data splits\n",
    "print(\"TRAIN DATA\")\n",
    "print(train_df.head(10))\n",
    "print('\\n**********************************************************************\\n')\n",
    "print(\"VALIDATION DATA\")\n",
    "print(val_df.head(10))\n",
    "print('\\n**********************************************************************\\n')\n",
    "print(\"TEST DATA\")\n",
    "print(test_df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bde100ea-8b31-45ef-a5fe-a0a396b78eb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n",
      "{'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n",
      "{'id': Value(dtype='int32', id=None), 'verse_text': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive', 'no_impact', 'mixed'], id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_ds.features)\n",
    "print(val_ds.features)\n",
    "print(test_ds.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d2219c-00d4-4544-b573-df459be5d4b6",
   "metadata": {},
   "source": [
    "### Tunstall, von Werra and Thomas Wolf: Natural Language Processing with Transformers --> Building Language Applications with Hugging Face (ch. 2)\n",
    "\n",
    "- DistilBERT cannot receive raw strings as input, the texts must be tokenized and \"encoded\".\n",
    "- Tokenization: breaking down a string into the atomic units used in the model.\n",
    "- The optimal splitting of words into subunits is **usually learned from the corpus**.\n",
    "- p.33 on subword tokenization, dealing with complex words/mispellings by combining the best aspects of character and word tokenization --> it is \"learned\" from the pre-training corpus using a mix of rule-based and statistical algorithms.\n",
    "- WordPiece is used by DistilBERT tokenizers.\n",
    "- \"HuggingFace Trans\n",
    "formers provides a convenient AutoTokenizer class that allows you to quickly loa \r\n",
    "the tokenizer associated with a pretrained model—we just call its from_pretrained )\r\n",
    "method, providing the ID of a model on the Hub or a local file path. Let’s start by\r\n",
    "loading the tokenizer for Distil.\" (p.33)\n",
    "- BRT \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e501085-87b6-458a-93c4-7e60c01d6629",
   "metadata": {},
   "source": [
    "### DistilBERT Link and Description\n",
    "- Link: https://huggingface.co/distilbert/distilbert-base-uncased\n",
    "- \"DistilBERT is a transformers model, smaller and faster than BERT, which was pretrained on the same corpus in a self-supervised fashion, using the BERT base model as a teacher. This means it was pretrained on the raw texts only, with no humans labelling them in any way (which is why it can use lots of publicly available data) with an automatic process to generate inputs and labels from those texts using the BERT base model.\"\n",
    "- \"Distillation loss: the model was trained to return the same probabilities as the BERT base model.\"\n",
    "- \"it's mostly intended to be fine-tuned on a downstream task.\"\n",
    "- \"Even if the training data used for this model could be characterized as fairly neutral, this model can have biased predictions.\"\n",
    "- \"DistilBERT pretrained on the same data as BERT, which is BookCorpus, a dataset consisting of 11,038 unpublished books and English Wikipedia (excluding lists, tables and headers).\"\n",
    "- The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form: \"[CLS] Sentence A [SEP] Sentence B [SEP]\"\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eeed08-554e-41eb-9d59-c835d734cebf",
   "metadata": {},
   "source": [
    "### Getting Started with Sentiment Analysis using Python\n",
    "\n",
    "https://huggingface.co/blog/sentiment-analysis-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09f3e0f-954a-47cd-bc19-41ca85761219",
   "metadata": {},
   "source": [
    "### Hyperparameter Search with Transformers and Ray Tune\n",
    "## Code adapted from: https://huggingface.co/blog/ray-tune\n",
    "\n",
    "https://huggingface.co/blog/ray-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05d2391f-1cdf-44db-bf9f-d61ade02bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44cb140-b828-4ec9-96e8-9187e8584246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE FROM: https://python.plainenglish.io/fine-tuning-distilbert-with-your-own-dataset-for-multi-classification-task-69f944189648\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a64311f-f5d2-47c7-b07c-de822c805a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                         verse_text  label\n",
      "0   0                      my canoe to make more steady,      2\n",
      "1   1  and be glad in the summer morning when the kin...      1\n",
      "2   2       and when they reached the strait symplegades      2\n",
      "3   3                             she sought for flowers      2\n",
      "4   4                       if they are hungry, paradise      2\n",
      "Maximum sequence length: 20\n",
      "Maximum sequence length: 27\n",
      "Maximum sequence length: 20\n"
     ]
    }
   ],
   "source": [
    "# Load in original train-val-test datasets and extract samples and labels\n",
    "train_set = pd.read_csv(\"original_test_df.csv\")\n",
    "val_set = pd.read_csv(\"original_val_df.csv\")\n",
    "test_set = pd.read_csv(\"original_test_df.csv\")\n",
    "print(train_set.head(5))\n",
    "# Convert from pandas Series to list as this is what the distilbert tokenizer requires as inputs\n",
    "train_texts = train_set[\"verse_text\"].to_list()\n",
    "train_labels=train_set[\"label\"].to_list()\n",
    "val_texts = val_set[\"verse_text\"].to_list()\n",
    "val_labels = val_set[\"label\"].to_list()\n",
    "test_texts = test_set[\"verse_text\"].to_list()\n",
    "test_labels = test_set[\"label\"].to_list()\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "# Find longest token length in dataset\n",
    "max_length = 0\n",
    "\n",
    "# Tokenize each text sample and find max length\n",
    "for text in train_set[\"verse_text\"]:\n",
    "    # Tokenize text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True) # special tokens = indicate start of sequence, end of sequence, separation\n",
    "    # Update max length\n",
    "    max_length = max(max_length, len(input_ids))\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "max_length = 0\n",
    "# Tokenize each val sample and find max length\n",
    "for text in val_set[\"verse_text\"]:\n",
    "    # Tokenize text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True) # special tokens = indicate start of sequence, end of sequence, separation\n",
    "    # Update max length\n",
    "    max_length = max(max_length, len(input_ids))\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)\n",
    "\n",
    "max_length = 0\n",
    "# Tokenize each val sample and find max length\n",
    "for text in test_set[\"verse_text\"]:\n",
    "    # Tokenize text\n",
    "    input_ids = tokenizer.encode(text, add_special_tokens=True) # special tokens = indicate start of sequence, end of sequence, separation\n",
    "    # Update max length\n",
    "    max_length = max(max_length, len(input_ids))\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "784fcfeb-c755-461c-b04d-a33170e45259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926a35e051df44048867c5124c57d8fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\ophel\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f2103811ad64a43942f4ec7598af886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac9704f2e3724d0aba9cca00584dbd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6b402f53904151a928d050ea0a06e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3eee30848f24849b73bbaaca91aa077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length for training set: 24\n",
      "Maximum sequence length for validation set: 24\n",
      "Maximum sequence length for test set: 24\n"
     ]
    }
   ],
   "source": [
    "# Initialize RoBERTa tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Function to tokenize and find maximum sequence length\n",
    "def tokenize_and_find_max_length(texts):\n",
    "    max_length = 0\n",
    "    for text in texts:\n",
    "        # Tokenize text\n",
    "        input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
    "        # Update max length\n",
    "        max_length = max(max_length, len(input_ids))\n",
    "    return max_length\n",
    "\n",
    "# Tokenize and find max length for train, validation, and test sets\n",
    "max_length_train = tokenize_and_find_max_length(train_texts)\n",
    "max_length_val = tokenize_and_find_max_length(val_texts)\n",
    "max_length_test = tokenize_and_find_max_length(test_texts)\n",
    "\n",
    "print(\"Maximum sequence length for training set:\", max_length_train)\n",
    "print(\"Maximum sequence length for validation set:\", max_length_val)\n",
    "print(\"Maximum sequence length for test set:\", max_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76760c53-db65-4694-b25d-abe5eca12ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "[101, 2010, 2132, 2003, 11489, 1012, 2002, 6732, 2006, 2273, 1998, 5465, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[CLS] of long - uncoupled bed, and childless eld, [SEP] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "## Code from: https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "\"\"\"\n",
    "    Fine-tuning in the HuggingFace's transformers library involves using a pre-trained model and a tokenizer \n",
    "    that is compatible with that model's architecture and input requirements.\n",
    "    Each pre-trained model in transformers can be accessed using the right model\n",
    "    class and be used with the associated tokenizer class. Since we want to use \n",
    "    DistilBert for a classification task, we will use the DistilBertTokenizer tokenizer \n",
    "    class to tokenize our texts and then use TFDistilBertForSequenceClassification \n",
    "    model class in a later section to fine-tune the pre-trained model using the output from the tokenizer.\n",
    "    The DistilBertTokenizer generates input_ids and attention_mask as outputs.\n",
    "    This is what is required by a DistilBert model as its inputs.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True)\n",
    "print(train_encodings.keys())\n",
    "# Code from: https://medium.com/@raoashish10/fine-tuning-a-pre-trained-bert-model-for-classification-using-native-pytorch-c5f33e87616e\n",
    "print(val_encodings['input_ids'][8])\n",
    "print(val_encodings['attention_mask'][8])\n",
    "print(tokenizer.decode(train_encodings['input_ids'][8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f80ab-0e54-439e-bc91-be437589acce",
   "metadata": {},
   "source": [
    "From https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "\n",
    "*\"So, in the above code, we defined the tokenizer object using the from_pretrained() method which downloads and caches the tokenizer files associated with the DistilBert model. When we pass text through this tokenizer the generated output will be in the format expected by the DistilBert architecture, as stated above. We use padding and truncation to make sure all the vectors are the same size. You can learn more about DistilBert and it's tokenizer from the DistilBert section of the transformers library's official documentation. And more info regarding the padding and truncation options is available here. Now that we have our texts in an encoded form, there is only one step left before we can begin the fine-tuning process.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc870b51-3f54-47b6-8cdc-ca6e6c65cff9",
   "metadata": {},
   "source": [
    "## Fine-Tuning using TFTrainer Class Provided by the *transformers* Library: enables easy training and evaluation of models\n",
    "\n",
    "\n",
    "From https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/\n",
    "- The TFTrainer (Trainer for Pytorch) is a class provided by the transformers library that offers a simple, yet feature-rich, method of training and evaluating models.\n",
    "- The following code shows how to define the configuration settings and build a model using the TFTrainer class.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0de4fccd-1d2d-423c-8674-160c2b51707f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my canoe to make more steady, 2 \n",
      "\n",
      "{'input_ids': tensor([  101,  2026, 14347,  2000,  2191,  2062,  6706,  1010,   102,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(2)}\n"
     ]
    }
   ],
   "source": [
    "# Code from: https://huggingface.co/transformers/v3.4.0/custom_datasets.html\n",
    "\"\"\" We put the data in this format so that the data can be easily batched such that each key in the batch encoding\n",
    "    corresponds to a named parameter of the forward() method of the model we will train. \"\"\"\n",
    "\n",
    "# Create a custom dataset class inheriting from PyTorch's Dataset class --> required as inputs to the DistilbertModel\n",
    "class PoemDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Store list of encoded tokens here\n",
    "        self.encodings = encodings\n",
    "        # Store list of corresponding labels for each sample here\n",
    "        self.labels = labels\n",
    "        \n",
    "    # This special __ getter function enables retrieving items in an encoding using []-notation and 'idx' as the integer to index into the \n",
    "    # encodings\n",
    "    def __getitem__(self, idx):\n",
    "        # dict comprehension: creates a key for each key in the encoding for a specific idx/sample: e.g., 'input_ids'\n",
    "        # the values will be the list containing the encoded tokens, attention masks etc.\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # add key-value pair for label of indexed sample\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    # return size of dataset\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = PoemDataset(train_encodings, train_labels)\n",
    "val_dataset = PoemDataset(val_encodings, val_labels)\n",
    "test_dataset = PoemDataset(test_encodings, test_labels)\n",
    "\n",
    "# Verify this has worked by taking the first train sample as an example\n",
    "print(train_texts[0], train_labels[0], '\\n')\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d20f2-84be-4a3e-a56e-231a1d13b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ref: https://medium.com/@rakeshrajpurohit/customized-evaluation-metrics-with-hugging-face-trainer-3ff00d936f99\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    true_labels = pred.label_ids\n",
    "    predicted_labels = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    # select metrics for dealing with unbalanced classes (macro precision/recall/f1-score)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=0.0)\n",
    "    return {\n",
    "        'eval_precision': precision,\n",
    "        'eval_recall': recall,\n",
    "        'eval_f1': f1,\n",
    "    }\n",
    "\n",
    "# Define hyperparameter search-space --> https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b\n",
    "# For fine-tuning, most model hyperparameters are\n",
    "#the same as in pre-training, with the exception of\n",
    "#the batch size, learning rate, and number of training epochs. The dropout probability was always\n",
    "#kept at 0.1. The optimal hyperparameter values\n",
    "#are task-specific, but we found the following range REF https://arxiv.org/pdf/1810.04805 --> \n",
    "#\"learning_rate\": [2e-5, 3e-5, 5e-5],\n",
    "#  \"num_epochs\": [2, 3, 4]\n",
    "# large datasets are less sensitive but small ones like this one are very sensitive\n",
    "# https://colab.research.google.com/drive/1tQgAKgcKQzheoh503OzhS4N9NtfFgmjF?usp=sharing#scrollTo=hGIax71xIDqy --> ideas for which hyperparam ranges\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "def train_fn(config):\n",
    "    # Define your Trainer initialization and training logic here\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./original_dataset_results',\n",
    "        per_device_train_batch_size=config[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=config[\"per_device_eval_batch_size\"],\n",
    "        warmup_steps=config[\"warmup_steps\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=config[\"num_train_epochs\"],\n",
    "        learning_rate=config[\"learning_rate\"]\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model\n",
    "    eval_results = trainer.evaluate(eval_dataset=val_dataset)\n",
    "\n",
    "\n",
    "# Define hyperparameter search space\n",
    "search_space = {\n",
    "    \"num_train_epochs\": tune.choice([6, 7, 8, 9, 10]), # lower epochs gave metrics results close to 0\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"per_device_eval_batch_size\": 16,\n",
    "    \"warmup_steps\": 500,  # Adjusted to use randint for warmup_steps\n",
    "    \"weight_decay\": tune.choice([0.001, 0.01, 0.1]),\n",
    "    \"learning_rate\": tune.choice([5e-5, 3e-5, 2e-5])\n",
    "}\n",
    "\n",
    "# Define PopulationBasedTraining scheduler\n",
    "scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    metric=\"f1-score\",\n",
    "    mode=\"max\",\n",
    "    perturbation_interval=1,\n",
    "    hyperparam_mutations={\n",
    "        \"weight_decay\": lambda: np.random.uniform(0.0, 0.3),\n",
    "        \"learning_rate\": lambda: np.random.uniform(1e-5, 5e-5),\n",
    "    })\n",
    "\n",
    "# Define a trial_dirname_creator to shorten the path\n",
    "def short_trial_dirname_creator(trial):\n",
    "    return f\"tune_trial_{trial.trial_id}\"\n",
    "\n",
    "# Run hyperparameter search with Ray Tune\n",
    "analysis = tune.run(\n",
    "    train_fn,\n",
    "    config=search_space,\n",
    "    scheduler=scheduler,\n",
    "    resources_per_trial={\"cpu\": 4},\n",
    "    num_samples=10,\n",
    "    progress_reporter=tune.CLIReporter(metric_columns=[\"f1-score\"]),\n",
    "    trial_dirname_creator=short_trial_dirname_creator\n",
    ")\n",
    "\n",
    "# Retrieve and print the best hyperparameters\n",
    "best_trial = analysis.get_best_trial(metric=\"f1-score\", mode=\"max\")\n",
    "print(\"Best hyperparameters found:\", best_trial.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f6e366c-27a2-4855-bbc7-fa9b7c662606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1, Params: {'learning_rate': 5e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:22, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.357684</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.184932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.361500</td>\n",
       "      <td>1.345888</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.246377</td>\n",
       "      <td>0.201183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.351800</td>\n",
       "      <td>1.324805</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.351800</td>\n",
       "      <td>1.290205</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.305600</td>\n",
       "      <td>1.242310</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.250100</td>\n",
       "      <td>1.180754</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.250100</td>\n",
       "      <td>1.104481</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.142600</td>\n",
       "      <td>1.028930</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2, Params: {'learning_rate': 5e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:25, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.351288</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.321833</td>\n",
       "      <td>0.362827</td>\n",
       "      <td>0.229117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.356400</td>\n",
       "      <td>1.338366</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.333079</td>\n",
       "      <td>0.400203</td>\n",
       "      <td>0.306389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.315868</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.318333</td>\n",
       "      <td>0.383677</td>\n",
       "      <td>0.347671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.282813</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.303600</td>\n",
       "      <td>1.243623</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.246900</td>\n",
       "      <td>1.196761</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.246900</td>\n",
       "      <td>1.138016</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.161300</td>\n",
       "      <td>1.068247</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3, Params: {'learning_rate': 5e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:26, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.351288</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>0.321833</td>\n",
       "      <td>0.362827</td>\n",
       "      <td>0.229117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.356400</td>\n",
       "      <td>1.338365</td>\n",
       "      <td>0.438095</td>\n",
       "      <td>0.333079</td>\n",
       "      <td>0.400203</td>\n",
       "      <td>0.306389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.315868</td>\n",
       "      <td>0.580952</td>\n",
       "      <td>0.318333</td>\n",
       "      <td>0.383677</td>\n",
       "      <td>0.347671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>1.282813</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.303600</td>\n",
       "      <td>1.243624</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.246900</td>\n",
       "      <td>1.196764</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.246900</td>\n",
       "      <td>1.138021</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.161300</td>\n",
       "      <td>1.068253</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4, Params: {'learning_rate': 3e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:26, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.352813</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.319296</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.206694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357400</td>\n",
       "      <td>1.345171</td>\n",
       "      <td>0.390476</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.388762</td>\n",
       "      <td>0.279825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.332089</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.328976</td>\n",
       "      <td>0.398932</td>\n",
       "      <td>0.326256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.312352</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.340196</td>\n",
       "      <td>0.398169</td>\n",
       "      <td>0.366411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.325900</td>\n",
       "      <td>1.286772</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.256616</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.221902</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.235100</td>\n",
       "      <td>1.180499</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5, Params: {'learning_rate': 3e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:25, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.352813</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.319296</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.206694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357400</td>\n",
       "      <td>1.345171</td>\n",
       "      <td>0.390476</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.388762</td>\n",
       "      <td>0.279825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.332089</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.328976</td>\n",
       "      <td>0.398932</td>\n",
       "      <td>0.326256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.312352</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.340196</td>\n",
       "      <td>0.398169</td>\n",
       "      <td>0.366411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.325900</td>\n",
       "      <td>1.286772</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.256616</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.221902</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.235100</td>\n",
       "      <td>1.180499</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 6, Params: {'learning_rate': 3e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:25, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.352813</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.319296</td>\n",
       "      <td>0.361048</td>\n",
       "      <td>0.206694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357400</td>\n",
       "      <td>1.345171</td>\n",
       "      <td>0.390476</td>\n",
       "      <td>0.332071</td>\n",
       "      <td>0.388762</td>\n",
       "      <td>0.279825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.332089</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>0.328976</td>\n",
       "      <td>0.398932</td>\n",
       "      <td>0.326256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.343200</td>\n",
       "      <td>1.312353</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.340196</td>\n",
       "      <td>0.398169</td>\n",
       "      <td>0.366411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.325900</td>\n",
       "      <td>1.286772</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.256617</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.289100</td>\n",
       "      <td>1.221905</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.235100</td>\n",
       "      <td>1.180503</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7, Params: {'learning_rate': 2e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:26, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.353574</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.327839</td>\n",
       "      <td>0.368930</td>\n",
       "      <td>0.197444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357800</td>\n",
       "      <td>1.348521</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.323921</td>\n",
       "      <td>0.377320</td>\n",
       "      <td>0.249194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>1.339931</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.324038</td>\n",
       "      <td>0.385711</td>\n",
       "      <td>0.289212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>1.327085</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.389779</td>\n",
       "      <td>0.340975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>1.310099</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.377574</td>\n",
       "      <td>0.349612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.288867</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.264193</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.272800</td>\n",
       "      <td>1.236474</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 8, Params: {'learning_rate': 2e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:31, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.353574</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.327839</td>\n",
       "      <td>0.368930</td>\n",
       "      <td>0.197444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357800</td>\n",
       "      <td>1.348521</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.323921</td>\n",
       "      <td>0.377320</td>\n",
       "      <td>0.249194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>1.339931</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.324038</td>\n",
       "      <td>0.385711</td>\n",
       "      <td>0.289212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>1.327085</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.389779</td>\n",
       "      <td>0.340975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>1.310099</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.377574</td>\n",
       "      <td>0.349612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.288867</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.264193</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.272800</td>\n",
       "      <td>1.236474</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 9, Params: {'learning_rate': 2e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\ophel\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='56' max='56' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [56/56 01:30, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.353574</td>\n",
       "      <td>0.276190</td>\n",
       "      <td>0.327839</td>\n",
       "      <td>0.368930</td>\n",
       "      <td>0.197444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.357800</td>\n",
       "      <td>1.348521</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.323921</td>\n",
       "      <td>0.377320</td>\n",
       "      <td>0.249194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>1.339930</td>\n",
       "      <td>0.409524</td>\n",
       "      <td>0.324038</td>\n",
       "      <td>0.385711</td>\n",
       "      <td>0.289212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.346700</td>\n",
       "      <td>1.327085</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.389779</td>\n",
       "      <td>0.340975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.336800</td>\n",
       "      <td>1.310099</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.377574</td>\n",
       "      <td>0.349612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.288867</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.386731</td>\n",
       "      <td>0.346046</td>\n",
       "      <td>0.295312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.310900</td>\n",
       "      <td>1.264193</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.272800</td>\n",
       "      <td>1.236475</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.219048</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.264368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7/7 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Define compute_metrics function\n",
    "def compute_metrics(pred):\n",
    "    true_labels = pred.label_ids\n",
    "    predicted_labels = pred.predictions.argmax(-1)\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    # Calculate precision, recall, F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=0.0)\n",
    "    return {\n",
    "        'eval_accuracy': accuracy,\n",
    "        'eval_precision': precision,\n",
    "        'eval_recall': recall,\n",
    "        'eval_f1': f1,\n",
    "    }\n",
    "\n",
    "# Define grid of hyperparameters --> experiment with different learning rates and ewight decay values\n",
    "param_grid = {\n",
    "    \"per_device_train_batch_size\": [16],  # Reduce batch size if memory issues\n",
    "    \"per_device_eval_batch_size\": [16],\n",
    "    \"learning_rate\": [5e-5, 3e-5, 2e-5],\n",
    "    \"num_train_epochs\": [8],\n",
    "    \"weight_decay\": [0.001, 0.01, 0.1],\n",
    "    \"warmup_steps\": [500],\n",
    "}\n",
    "\n",
    "# Initialize variables to track the best hyperparameters and scores\n",
    "best_hyperparams = None\n",
    "best_macro_f1 = 0.0\n",
    "\n",
    "counter=1\n",
    "# Iterate over each set of hyperparameters\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Trial {counter}, Params: {params}\")\n",
    "    \n",
    "    # Re-initialize the model for each set of hyperparameters\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4)\n",
    "    \n",
    "    # Define training arguments with current hyperparameters\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./original_ds_results',\n",
    "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
    "        per_device_eval_batch_size=params[\"per_device_eval_batch_size\"],\n",
    "        warmup_steps=params[\"warmup_steps\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "        logging_dir='./original_ds_logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",  # This is important for Trainer to call compute_metrics after each epoch\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        num_train_epochs=params[\"num_train_epochs\"],\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer with current hyperparameters\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    eval_result = trainer.evaluate(eval_dataset=val_dataset)\n",
    "\n",
    "    # Track the best hyperparameters based on evaluation metric (e.g., F1-score)\n",
    "    if eval_result[\"eval_f1\"] > best_macro_f1:\n",
    "        best_macro_f1 = eval_result[\"eval_f1\"]\n",
    "        best_hyperparams = params\n",
    "\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d42ad618-74ca-4f7c-bddb-8bd40d15866e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparams: {'learning_rate': 5e-05, 'num_train_epochs': 8, 'per_device_eval_batch_size': 16, 'per_device_train_batch_size': 16, 'warmup_steps': 500, 'weight_decay': 0.001}\n",
      "Best macro-average F1-score: 0.26436781609195403\n"
     ]
    }
   ],
   "source": [
    "# Print the best hyperparameters found\n",
    "print(\"Best hyperparams:\", best_hyperparams)\n",
    "print(\"Best macro-average F1-score:\", best_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4c2e5-dd5d-42e3-aa14-7723d19a4813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
